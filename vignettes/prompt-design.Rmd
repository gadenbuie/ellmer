---
title: "Prompt design"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Prompt design}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = elmer:::openai_key_exists()
)
```

If you've never written an LLM prompt before, this vignette is going to give you the basics to

If you've never written a prompt before, the easiest way to think about it is precisely explaining what you want a technically skilled but naive human what you want. The key is clearly stating what you want, resolving any potential ambiguities, and providing a few examples. And indeed writing the prompt can often help you too, because it forces you to be explicit about what you want. You should expect to iterate multiple times before you get to a good prompt, but you'll learn a lot along the way. It's good practice to keep your prompt in Git (so you can record exactly how it's changing) and build up a small set of challenge examples that you can use to verify that the prompt does what you expect. (Eventually, you might want to formally __evaluate__ different prompts for the problem you're tackling, but that's currently outside the scope of elmer and this vignette).

Time-constrained human. Clarity and concision help in the same way that they help humans.

It's also a good idea to read the advice for the specific model that you're using, as there may be specific tweaks that work well with just that model. Here are some pointers to the prompt engineering guides for a few popular models:

* [Claude](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)
* [OpenAI](https://platform.openai.com/docs/guides/prompt-engineering)
* [Gemini](https://ai.google.dev/gemini-api/docs/prompting-intro)

```{r setup}
library(elmer)
```

## Prompt examples

Because prompts can be quite long, we suggest writing them in markdown. That way you can use headers to divide up the prompt, and other tools like itemised lists to enumerate multiple options.

* Shiny assistant prompt
* Hadley recipe prompt
* Sidebot
* https://github.com/simonpcouch/pal/tree/main/inst/prompts

## Directing behavior/output

### Basic flavour

```{r}
chat <- chat_openai(system_prompt = "Be extremely terse. Don't even use punctuation.")
chat$chat("What's the capital of New Zealand?")
```

```{r}
chat <- chat_openai(system_prompt = "
  Be loquacious.
  Enjoy using florid turns of phrase.
  Drop in extra historical information even when not asked for it.
")
chat$chat("What's the capital of New Zealand?")
```

```{r}
chat <- chat_openai(system_prompt = "
  You are an expert R programmer.
  Just give me the code; no explanation in text.
  But include a few comments in line explaining particularly complex operations.
")
chat$chat("
  How can I compute the mean and median of variables a, b, c, ...
  grouped by age and sex
")
```

### Be clear and specific

If you're not getting the results that you're looking for, you might need to provide more context.

```{r}
chat <- chat_openai(system_prompt = "
  You are an expert R programmer.
  You prefer to use the tidyverse when you can.
  You follow the tidyverse style guide.
  Use the base pipe, not magittr.
  Just give me the code; no explanation.
")
chat$chat("
  How can I compute the mean and median of variables a, b, c, ...
  grouped by age and sex
")
chat$chat("Can you use across?")
```

```{r}
chat <- chat_openai(system_prompt = "
  You are an expert R programmer.
  You prefer to use base R. Don't use the tidyverse.
  Just give me the code; no explanation.
")
chat$chat("
  How can I compute the mean and median of variables a, b, c, ...
  grouped by age and sex. Return the results in a single data frame.
")
```

### Teach it about new features

```{r}
chat <- chat_openai(system_prompt = "
  dplyr 1.1.0 introduced per-operation grouping with the `.by` argument.
  You job is to convert code that uses group_by() to code that uses `.by`.

  Some examples:

  # Before
  transactions |>
    group_by(company, year) |>
    mutate(total = sum(revenue))

  # After
  transactions |>
    mutate(
      total = sum(revenue),
      .by = c(company, year)
    )
")
chat$chat("
  How can I compute the mean and median of variables a, b, c, ...
  grouped by age and sex. Just show me the code.
")

```

Examples are a powerful technique that we'll discuss in more detail below.

## Mixing in data

`interpolate()` uses glue syntax, but instead of single braces it uses double braces. This more closes matches example you'll see elsewhere in the world and make it a bit easier to use it with JSON.

## Specific techniques

### Chain of thought

You can often get better responses if you ask the LLM to explain it's work, or begin by asking it to come up with a plan of attack. Because these answers are included in the chat history, and resent to the model on each turn, the assistant can use it's own explanations to improve future answers.

### Uncertainty


- "Be careful to only provide answers that you are sure about. If you are uncertain about an answer, say so."

### Other tools

If you have a claude account, you can use its <https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator>. This prompt generator has been specifically tailored for Claude, but I suspect it will help many other LLMs, or at least give you some ideas as to what else you might want to include in your prompt.


## Using examples to guide behavior

You'll also hear this called single-shot or multi-shot prompting.

Note, however, there are some

- **Goal:** Extract ingredient list from recipe and return in a structured format.

- Example user input:

  ```
  In a large bowl, cream together 1 cup of softened unsalted butter and ½ cup of white sugar until smooth. Beat in 1 egg and 1 teaspoon of vanilla extract. Gradually stir in 2 cups of all-purpose flour until the dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C) for 10-12 minutes, or until the edges are lightly browned. Let the cookies cool on the baking sheet for a few minutes before transferring to a wire rack to cool completely. Enjoy!
  ```


### System prompt 1: just instructions

```
The user input contains a recipe. Extract a list of ingredients and return it in JSON format.
```

Assistant response:

```{.json}
{
  "ingredients": [
    "1 cup of softened unsalted butter",
    "½ cup of white sugar",
    "1 egg",
    "1 teaspoon of vanilla extract",
    "2 cups of all-purpose flour",
    "1 cup of semisweet chocolate chips"
  ]
}
```

### System prompt 2: detailed description of output

```
The user input contains a recipe. Extract a list of ingredients and return it in JSON format. It should be an array of objects, where each object has keys `ingredient`, `quantity`, and `unit`. Put each object on one line of output.
```

Assistant response:

```{.json}
[
    {"ingredient": "unsalted butter", "quantity": 1, "unit": "cup"},
    {"ingredient": "white sugar", "quantity": 1/2, "unit": "cup"},
    {"ingredient": "egg", "quantity": 1, "unit": "large"},
    {"ingredient": "vanilla extract", "quantity": 1, "unit": "teaspoon"},
    {"ingredient": "all-purpose flour", "quantity": 2, "unit": "cups"},
    {"ingredient": "semisweet chocolate chips", "quantity": 1, "unit": "cup"}
]
```

### System prompt 3: one-shot example

````
The user input contains a recipe. Extract a list of ingredients and return it in JSON format.

Example Output:

```json
[
    { "ingredient": "Flour", "quantity": 1, "unit": "cup" },
    { "ingredient": "Vegetable oil", "quantity": 0.5, "unit": "tsp" },
    { "ingredient": "Onion", "quantity": 1, "unit": null },
]
```
````

Assistant response:

```{.json}
[
    { "ingredient": "Unsalted butter", "quantity": 1, "unit": "cup" },
    { "ingredient": "White sugar", "quantity": 0.5, "unit": "cup" },
    { "ingredient": "Egg", "quantity": 1, "unit": null },
    { "ingredient": "Vanilla extract", "quantity": 1, "unit": "teaspoon" },
    { "ingredient": "All-purpose flour", "quantity": 2, "unit": "cups" },
    { "ingredient": "Semisweet chocolate chips", "quantity": 1, "unit": "cup" }
]
```

## Adding context/knowledge to prompt

- Add documentation files to prompt
- Add positive examples (negative examples don't work well)
- Docs must fit in context window
- Examples
  - [Elmer assistant](https://github.com/jcheng5/elmer-assistant/blob/main/prompt.generated.md) uses README files in prompt
  - [Sidebot](https://github.com/jcheng5/py-sidebot/blob/main/prompt.md)
  - [FastHTML LLM prompt](https://docs.fastht.ml/llms-ctx.txt)
