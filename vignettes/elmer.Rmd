---
title: "Getting started with elmer"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting started with elmer}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(elmer)
```

The goal of elmer is to make it easier to access to the wealth of large language models from R. But what can you do with those models once you have them? The goal of this vignette is to give you the basic vocabulary you need in order to use them effectively, and show a bunch of examples of the types of thing that you can do with them to help get your creative juices flowing.

Here we'll ignore how LLMs actually work, focussing on them as useful black boxes. If you want to get a sense of how they work, we recommend watching Jeremy Howard's posit::conf(2023) keynote: [A hackers guide to open source LLMs](https://www.youtube.com/watch?v=sYliwvml9Es).

## Vocabulary

We'll start by laying out some key vocab that you'll need to understand LLMs. Unfortunately the vocab is all a little entangled, so to understand one term you have to know a little about some of the others. So we'll start with some simple definitions of the most important terms and then iteratively go a little deeper.

It all starts with a **prompt**, the text that you send to the LLM. This kicks off the **conversation**, which consistst of turns that alternate between your prompts and the model's responses. Behind the scenes, the request and response are represented by a sequence of **tokens**, which represent either individual words or components of each word. The tokens are used to compute the cost of using a model and the size of the **context window**, which is the total request and responses used to generate the response.

### What is a token?

An LLM is a model that predicts the most likely next word given a sequence of words. While much more complicated that a linear model, they still have the same fundamental underlying representation: a numeric matrix. That means we need some way to convert words to numbers, which is the goal of the **tokenizer**.

The tokenizer is inextricably tied to the model, so varies from model to model however. Fortunately, however, the details aren't too important, except that you should know that on average, an English word requires around 1.5 tokens (common words will be represented by a single token; rarer words will require multiple). A page might be 375-400 tokens. A complete book might be 75,000 to 150,000 tokens. Other languages will typically require more tokens, because LLMs are trained on data from the internet, which is primarily in English. With elmer, you can see how many tokens a conversations has used when you print it, or you can see total usage for a session with `token_usage()`.

Models are priced according to input and output tokens. Models are priced per million tokens and vary based on how much computation a model requires. Mid-teir models (e.g. gpt-4o or claude 3 haiku) might be around $0.25 per million input and $1 per million output tokens; state of the art models (like gpt-4o or claude 3 sonnet) might be more like $2.50 per million input tokens, and $10 per million output tokens. Certainly, at the time of writing, even $10 of API credit will give you a lot of room for experimenting when using mid-tier models. And prices are likely to continue to decline as model performance improves.

Tokens are also used to measure the "context length", which is how much data the LLM can use to predict the next response. As we'll discuss shortly, the context length includes the full state of your conversation so far (both your prompts and the model's responses), which means that costs grow quadratically with the length of conversation.

If you want to learn more about tokens and tokenizers, I'd recommend watching the first 20-30 minutes of [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) by Andrej Karpathy. You certainly don't need to learn how to build your own tokenizer, but the intro will give you a bunch of useful background knowledge that will help improve your undersstanding of how LLM's work.

### What is a conversation?

A conversation with an LLM takes place through a series of HTTP requests and responses: you send your question to the LLM in a HTTP request, and it sends its reply back in a HTTP response. A conversation always consists of a sequence of a pair turns: you send a prompt and then the model returns a response.

Depsite the fact that conversations are inherently stateful (i.e. your response to a question depends on the complete history of the conversation), LLM APIs are stateless. That means every time you send an question to an LLM, you have to actually send the entire conversation history. This is important to understand because:

* It affects pricing. You are charged per token, so each question in a conversation is going to include all the previous questions and answers, meaning that the cost is going to grow rapidly with the number of turns. In other words: to save money, keep your conversations short.

* You have full control over the conversational history. For example, this means that you can start a conversation with one model then send it to another. Or you can even make up responses that no model actually generated.

* Every response is affected by all previous questions and responses. This can make a converstion get stuck in a local optima, so in general, it's typically better to iterate by starting a new conversation with an improved prompt rather than iterating by having a long conversation with the model.

### What is a prompt?

As you've learned, the prompt is the text that you send to the model. But there's actually a layered sequence of prompts, starting with the **core system prompt**. These unchangeable, set by the model provider, and affect every conversation. You can these look like from Anthropic, who [publishes their core system prompts](https://docs.anthropic.com/en/release-notes/system-prompts).

The next layer is the **system prompt** that you set when creating a new conversation. This is a set of instructions that you provide to the model that will affect every turn in the conversation. For example, you might tell the model to always respond in Spanish or encourage it to use dependency-free base R whenever it writes R code.

Finally, we have the **user prompt** which is the text that's sent in each turn of the conversation.

Writing good prompts is called __prompt design__ and is key to effective use of LLMs. You can learn more about it `vignette("prompt-design")`.
Depending on what sort of tool you might focus your prompt design energy on either the system prompt or the user prompt. For example, if you're developing an app, then you'll work on the user prompt that helps frame the answers generated by your user's prompts. If you're working on structured data extraction, there's little difference between using the system prompt or the user prompt.

### Providers and models

A provider is web API that provides access to one or more models. Some providers are synonynous with a model: for example, OpenAI and chatGPT, Anthropic and Claude, and Google and Gemini. But other providers host many different models, typically the open source models like LLaMa and mistral. Still other providers do both, typically by partnering with a company that provides a popular closed model. For example, Azure OpenAI offers both open source models and OpenAI's chatGPT, and AWS Bedrock offers both open source models and Anthropic's Claude models.

## Example uses

Now that you've got the basic vocab under your belt, I'm going to just fire a bunch of interesting potential use cases at you. For many of these examples there are often special purpose tools that will be faster and cheaper. But using an LLM allows you to rapidly prototype an idea on a small subset of the full problem to determine if it's worth investing more time and effort.

### Chatbots

Great place to start is building a chatbot with a custom prompt. Chatbots are familiar interface and easy to create in R with [shinychat](https://github.com/jcheng5/shinychat).

You could create a chat bot to answer questions on a specific topic by   filling the prompt with related content. For example, maybe you want to help people use your new package. The default prompt won't work because LLMs don't know anything about your package. You can get surprisingly far by preloading the prompt with your README and other vignettes. This is how the [elmer assistant](https://github.com/jcheng5/elmer-assistant) works.

An even more complicated chat bot is [shiny assistant](https://shiny.posit.co/blog/posts/shiny-assistant/) which helps you build shiny apps (either in R or python). It combines a [prompt](https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt.md) that gives general advice with a language specific prompt for [R](https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_r.md) or [python](https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_python.md). The python prompt is very detailed because there's much less information about shiny for Python on the internet because it's a much newer package.

Another direction is to give the chat bot additional context about your current environment. For example, [aidea](https://github.com/cpsievert/aidea) allows the user to interactively explore a dataset with the help of the LLM. It adds summary statistics about the dataset to the [prompt](https://github.com/cpsievert/aidea/blob/main/inst/app/prompt.md) so that the LLM has context about the dataset. If you were working on a chatbot to help the user read in data, you could imagine include all the files in the current directory along with their first few lines.

Generally, there's a surprising amount of value to creating a chatbot that has a prompt containing data that's already freely available on the internet. At best, search often only gets to you the correct page, whereas a chat bot can answer a specific narrowly scoped question. If you have more context than can be stuffed in a prompt, you'll need to use some other technique like RAG (retrieval-augmented generation). I'm planning to work on an R package to make that easier shortly.

### Structured data extraction

LLMs can be very good at extracting structured data from unstructured text. Do you have any raw data that you've struggled to analyse in the past because it's just big wodges of plain text? Read `vignette("structure-data")` to learn about how you can use it.

Some examples:

* I've extracted structured recipe data from baking and cocktail recipes. Once you have the data in a structured form you can use your R skills to better understand how (e.g.) recipes vary within a cookbook. Or you could look for recipes that use the ingredients that you currently have in your kitchen.

* Extract key details from customer tickets or GitHub issues. You can use LLMs for quick and dirty sentiment analysis, extract any specific products mentioned, and summarise the discussion into a few bullet points.

* Structured data extraction also work works with images. It's not the fastest or cheapest way to extract data but it makes it really easy to prototype ideas. For example, maybe you have a bunch of scanned documents that you want to index. You can convert PDFs to images (e.g. using {imagemagick}) then use structured data extraction to pull out key details.

### Programming

Create a long hand written prompt that teaches the LLM about something it wouldn't otherwise know about. For example, you might write a guide to updating code to use a new version of a package. You could combine this with the rstudioapi package to allow the user to select code, transform it, and then replace the existing text. A comprehensive example of this sort of app is [pal](https://simonpcouch.github.io/pal/). It includes prompts for automatically generating a roxygen documentation block, updating testthat code to the 3rd edition, and converting `stop()` and `abort()` to use `cli::cli_abort()`.

You can also explore automatically adding addition context to the prompt. For example, you could automatically look up the documentation for an R function, and include it in the prompt.

You can use LLMs to explain code, or even ask them to [generate a diagram](https://bsky.app/profile/daviddiviny.bsky.social/post/3lb6kjaen4c2u).

### First pass

For more complicated problems, you may find that an LLM rarely generates a 100% correct solution. That can be ok, if you adopt the mindset of using the LLM to get started, solving the "blank page problem":

* Use your existing company style guide to generate a [brand.yaml](https://posit-dev.github.io/brand-yml/articles/llm-brand-yml-prompt/) specification to automatically style your reports, apps, dashboards, plots to match your corporate style guide. Using a prompt here is unlikely to give you perfect results, but it's likely to get you close and then you can manually iterate.

* I sometimes find it useful to have an LLM document a function for me, even knowing that it's likely to be mostly incorrect. It can often be much easier to react to some existing text than having to start completely from scratch.

* If you're workign with code or data from another programming language, you ask an LLM to convert it to R code for you. Even if it's not perfect, it's still typically much faster than doing everything yourself.
